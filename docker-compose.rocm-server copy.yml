services:
  llama-server:
    build:
      context: .
      dockerfile: Dockerfile.rocm
    image: llama-cpp-rocm
    security_opt:
        - seccomp:unconfined
    ipc: "host"
    shm_size: "8G"    
    container_name: llama-server-rocm
    ports:
      - "${SERVER_PORT:-8000}:8000"
    runtime: amd
    privileged: true
    volumes:
      - ./models:/models
      - ./models:/workspace/llama.cpp/models
      - ./output:/workspace/output
    environment:
      - ROCM_PATH=/opt/rocm-6.4.3
      - AMD_VISILBE_DEVICES=all
      - HIP_VISIBLE_DEVICES=${HIP_VISIBLE_DEVICES:-0,1,2,3,4,5,6,7}
      - GPU_MEMORY=${GPU_MEMORY:-auto}
      - MODEL_PATH=${MODEL_PATH:-/models/model.gguf}
      - CONTEXT_SIZE=${CONTEXT_SIZE:-2048}
      - GPU_LAYERS=${GPU_LAYERS:-999}
      - BATCH_SIZE=${BATCH_SIZE:-512}
      - SERVER_HOST=${SERVER_HOST:-0.0.0.0}
      - SERVER_PORT_INTERNAL=8000
      - LLAMA_DEBUG=${LLAMA_DEBUG:-0}
    working_dir: /workspace/llama.cpp
    env_file:
      - .env
    command: >
      /bin/bash -c '
        echo "amd-smi monitor output:";
        amd-smi monitor; 
        echo "PWD: $PWD";
        echo "Model: ${MODEL_PATH:-/models/model.gguf}";
        echo "Context Size: ${CONTEXT_SIZE:-2048}";
        echo "GPU Layers: ${GPU_LAYERS:-999}";
        echo "Batch Size: ${BATCH_SIZE:-512}";
        echo "Server Host: ${SERVER_HOST:-0.0.0.0}";
        echo "Server Port: ${SERVER_PORT_INTERNAL:-8000}";
        echo "Server will be available at http://localhost:${SERVER_PORT:-8000}";
        echo "---";
        echo "Environment variables:";
        env | grep -E "(MODEL_PATH|CONTEXT_SIZE|GPU_LAYERS|BATCH_SIZE|SERVER_|AMD)" | sort;
        echo "---";
        if [ ! -f "${MODEL_PATH:-/models/model.gguf}" ]; then
          echo "ERROR: Model file not found at ${MODEL_PATH:-/models/model.gguf}";
          echo "Please ensure your model is available in the ./models directory";
          echo "Available files:";
          ls -la /models/ || echo "No models directory mounted";
          exit 1;
        fi;
        echo "Model file found, starting server...";
        echo "Command that will be executed:";
        echo "/usr/local/bin/llama-cpp/llama-server --model \"${MODEL_PATH:-/models/model.gguf}\" --ctx-size ${CONTEXT_SIZE:-2048} --n-gpu-layers ${GPU_LAYERS:-999} --batch-size ${BATCH_SIZE:-512} --host ${SERVER_HOST:-0.0.0.0} --port ${SERVER_PORT_INTERNAL:-8000} --metrics --no-warmup --verbose";
        echo "---";
        exec /usr/local/bin/llama-cpp/llama-server \
          --model "${MODEL_PATH:-/models/model.gguf}" \
          --ctx-size ${CONTEXT_SIZE:-2048} \
          --n-gpu-layers ${GPU_LAYERS:-999} \
          --batch-size ${BATCH_SIZE:-512} \
          --host ${SERVER_HOST:-0.0.0.0} \
          --port ${SERVER_PORT_INTERNAL:-8000} \
          --metrics \
          --no-warmup \
          --verbose
      '