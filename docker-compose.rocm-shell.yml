services:
  llama-shell:
    build:
      context: .
      dockerfile: Dockerfile.rocm
    image: llama-cpp-rocm
    container_name: llama-cpp-shell
    runtime: amd
    volumes:
      - ./models:/models
      - ./output:/workspace/output
    environment:
      - ROCM_PATH=/opt/rocm-6.4.3
      - HIP_VISIBLE_DEVICES=${HIP_VISIBLE_DEVICES:-0}
      - GPU_MEMORY=${GPU_MEMORY:-auto}
    working_dir: /workspace/llama.cpp
    command: /bin/bash